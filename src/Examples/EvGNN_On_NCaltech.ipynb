{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load torch device",
   "id": "2bad99e408fd58a9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T09:14:52.999001Z",
     "start_time": "2025-11-15T09:14:52.983095Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam, AdamW\n",
    "\n",
    "from Datasets.batching import BatchManager\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "84be8768809da1d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model Initialization",
   "id": "d03bc1f7f31c429"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T09:14:55.649390Z",
     "start_time": "2025-11-15T09:14:53.100670Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from Models.CleanAEGNN.GraphRes import GraphRes as AEGNN\n",
    "from torch_geometric.data import Data as PyGData\n",
    "from Datasets.ncaltech101 import NCaltech\n",
    "\n",
    "image_size: tuple[int, int] = NCaltech.get_info().image_size\n",
    "input_shape: tuple[int, int, int] = (*image_size, 3)\n",
    "\n",
    "aegnn = AEGNN(\n",
    "    input_shape = input_shape,\n",
    "    kernel_size = 8,\n",
    "    n = [1, 16, 32, 32, 32, 128, 128, 128],\n",
    "    pooling_outputs = 128,\n",
    "    num_outputs = len(NCaltech.get_info().classes),\n",
    ").to(device)\n",
    "\n",
    "def transform_graph(graph: PyGData) -> PyGData:\n",
    "    graph = aegnn.data_transform(\n",
    "        graph, n_samples = 25000, sampling = True,\n",
    "        beta =  0.5e-5, radius = 5.0,\n",
    "        max_neighbors = 32\n",
    "    ).to(device)\n",
    "    return graph"
   ],
   "id": "37dbc3b654925cd3",
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "604f0ac3da13cf96",
   "metadata": {},
   "source": "# Dataset Initialization and processing (from the parsed dataset from the aegnn issues thread)"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-15T09:14:55.930618Z",
     "start_time": "2025-11-15T09:14:55.665395Z"
    }
   },
   "source": [
    "#Instantiating the ncaltech dataset\n",
    "ncaltech = NCaltech(\n",
    "    root=r\"\"\n",
    "    #root=r\"D:\\Uniwersytet\\GNNBenchmarking\\Datasets\\NCaltech\",\n",
    "    transform=transform_graph\n",
    ")\n",
    "\n",
    "# Processing the training part of the dataset\n",
    "ncaltech.process(modes = [\"training\"])"
   ],
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] Das System kann den angegebenen Pfad nicht finden: 'D:\\\\Uniwersytet\\\\GNNBenchmarking\\\\Datasets\\\\NCaltech\\\\Caltech101_annotations'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m#Instantiating the ncaltech dataset\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m ncaltech \u001B[38;5;241m=\u001B[39m \u001B[43mNCaltech\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mroot\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43mr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mD:\u001B[39;49m\u001B[38;5;124;43m\\\u001B[39;49m\u001B[38;5;124;43mUniwersytet\u001B[39;49m\u001B[38;5;124;43m\\\u001B[39;49m\u001B[38;5;124;43mGNNBenchmarking\u001B[39;49m\u001B[38;5;124;43m\\\u001B[39;49m\u001B[38;5;124;43mDatasets\u001B[39;49m\u001B[38;5;124;43m\\\u001B[39;49m\u001B[38;5;124;43mNCaltech\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtransform\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtransform_graph\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[43m)\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m# Processing the training part of the dataset\u001B[39;00m\n\u001B[0;32m      8\u001B[0m ncaltech\u001B[38;5;241m.\u001B[39mprocess(modes \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtraining\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n",
      "File \u001B[1;32m~\\Documents\\Hannes\\Uni\\Maastricht\\Project\\GNNBenchmark\\src\\Datasets\\ncaltech101.py:25\u001B[0m, in \u001B[0;36mNCaltech.__init__\u001B[1;34m(self, root, transform, pre_transform, pre_filter)\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__init__\u001B[39m(\n\u001B[0;32m     18\u001B[0m     \u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m, root: Union[\u001B[38;5;28mstr\u001B[39m, os\u001B[38;5;241m.\u001B[39mPathLike],\n\u001B[0;32m     19\u001B[0m     transform: Callable[[PyGData], PyGData] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m     20\u001B[0m     pre_transform: Callable[[PyGData], PyGData] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m     21\u001B[0m     pre_filter: Callable[[PyGData], \u001B[38;5;28mbool\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     22\u001B[0m ):\n\u001B[0;32m     23\u001B[0m     data_root \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(root, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCaltech101_annotations\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     24\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclasses \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msorted\u001B[39m(\n\u001B[1;32m---> 25\u001B[0m         [d \u001B[38;5;28;01mfor\u001B[39;00m d \u001B[38;5;129;01min\u001B[39;00m \u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlistdir\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_root\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mif\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39misdir(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(data_root, d))]\n\u001B[0;32m     26\u001B[0m     )\n\u001B[0;32m     28\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\n\u001B[0;32m     29\u001B[0m         root \u001B[38;5;241m=\u001B[39m root,\n\u001B[0;32m     30\u001B[0m         transform \u001B[38;5;241m=\u001B[39m transform,\n\u001B[0;32m     31\u001B[0m         pre_transform \u001B[38;5;241m=\u001B[39m pre_transform,\n\u001B[0;32m     32\u001B[0m         pre_filter \u001B[38;5;241m=\u001B[39m pre_filter\n\u001B[0;32m     33\u001B[0m     )\n\u001B[0;32m     34\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mx\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [WinError 3] Das System kann den angegebenen Pfad nicht finden: 'D:\\\\Uniwersytet\\\\GNNBenchmarking\\\\Datasets\\\\NCaltech\\\\Caltech101_annotations'"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "7c20608eb8eb604a",
   "metadata": {},
   "source": [
    "Display example events data point"
   ]
  },
  {
   "cell_type": "code",
   "id": "93d830e4e6b94432",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T09:14:55.946499200Z",
     "start_time": "2025-11-12T22:41:07.460510Z"
    }
   },
   "source": [
    "training_set = BatchManager(\n",
    "    dataset=ncaltech,\n",
    "    batch_size=8,\n",
    "    mode=\"training\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T09:14:55.957423700Z",
     "start_time": "2025-11-12T22:41:07.474448Z"
    }
   },
   "cell_type": "code",
   "source": [
    "optimizer = Adam(aegnn.parameters(), lr=5e-5)\n",
    "loss_fn = CrossEntropyLoss()\n",
    "\n",
    "classes = ncaltech.get_info().classes\n",
    "\n",
    "cls_to_idx = dict(zip(classes, range(len(classes))))"
   ],
   "id": "eac150840f2ad6de",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T09:14:55.957423700Z",
     "start_time": "2025-11-12T22:41:07.483252Z"
    }
   },
   "cell_type": "code",
   "source": [
    "aegnn.train()\n",
    "for i in range(200):\n",
    "    examples = next(training_set)\n",
    "    reference = torch.tensor([cls_to_idx[cls] for cls in examples.label], dtype=torch.long).to(device)\n",
    "    out = aegnn(examples)\n",
    "    loss = loss_fn(out, reference)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Iteration {i} loss: {loss.item()}\")\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "torch.save(aegnn.state_dict(), \"aegnn_ncaltech.pth\")"
   ],
   "id": "1a9630d19da94cf9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 loss: 17.573427200317383\n",
      "Iteration 1 loss: 17.83142852783203\n",
      "Iteration 2 loss: 9.329046249389648\n",
      "Iteration 3 loss: 11.488554954528809\n",
      "Iteration 4 loss: 9.886789321899414\n",
      "Iteration 5 loss: 9.645904541015625\n",
      "Iteration 6 loss: 8.793233871459961\n",
      "Iteration 7 loss: 7.6187944412231445\n",
      "Iteration 8 loss: 9.701828002929688\n",
      "Iteration 9 loss: 8.32140064239502\n",
      "Iteration 10 loss: 9.481127738952637\n",
      "Iteration 11 loss: 8.300086975097656\n",
      "Iteration 12 loss: 8.616653442382812\n",
      "Iteration 13 loss: 9.078987121582031\n",
      "Iteration 14 loss: 7.399202346801758\n",
      "Iteration 15 loss: 4.016829490661621\n",
      "Iteration 16 loss: 7.69382905960083\n",
      "Iteration 17 loss: 7.873544692993164\n",
      "Iteration 18 loss: 5.749243259429932\n",
      "Iteration 19 loss: 6.941044330596924\n",
      "Iteration 20 loss: 6.451145648956299\n",
      "Iteration 21 loss: 8.85360336303711\n",
      "Iteration 22 loss: 6.640111923217773\n",
      "Iteration 23 loss: 7.282352924346924\n",
      "Iteration 24 loss: 4.321983337402344\n",
      "Iteration 25 loss: 4.7192816734313965\n",
      "Iteration 26 loss: 5.341638565063477\n",
      "Iteration 27 loss: 5.869039535522461\n",
      "Iteration 28 loss: 4.179531097412109\n",
      "Iteration 29 loss: 5.073750972747803\n",
      "Iteration 30 loss: 5.238159656524658\n",
      "Iteration 31 loss: 6.82322883605957\n",
      "Iteration 32 loss: 4.825620651245117\n",
      "Iteration 33 loss: 5.554012298583984\n",
      "Iteration 34 loss: 3.7233805656433105\n",
      "Iteration 35 loss: 5.323196887969971\n",
      "Iteration 36 loss: 7.362215042114258\n",
      "Iteration 37 loss: 9.062414169311523\n",
      "Iteration 38 loss: 4.402960777282715\n",
      "Iteration 39 loss: 6.062992572784424\n",
      "Iteration 40 loss: 3.3208324909210205\n",
      "Iteration 41 loss: 6.049643039703369\n",
      "Iteration 42 loss: 4.36907958984375\n",
      "Iteration 43 loss: 5.7676191329956055\n",
      "Iteration 44 loss: 5.442407608032227\n",
      "Iteration 45 loss: 4.232250213623047\n",
      "Iteration 46 loss: 5.8231706619262695\n",
      "Iteration 47 loss: 4.483944892883301\n",
      "Iteration 48 loss: 4.7239990234375\n",
      "Iteration 49 loss: 3.6024839878082275\n",
      "Iteration 50 loss: 4.037508487701416\n",
      "Iteration 51 loss: 4.874760150909424\n",
      "Iteration 52 loss: 3.261237859725952\n",
      "Iteration 53 loss: 3.365514039993286\n",
      "Iteration 54 loss: 5.422002792358398\n",
      "Iteration 55 loss: 5.47874641418457\n",
      "Iteration 56 loss: 3.6800358295440674\n",
      "Iteration 57 loss: 4.026496410369873\n",
      "Iteration 58 loss: 4.8428850173950195\n",
      "Iteration 59 loss: 5.976591110229492\n",
      "Iteration 60 loss: 4.919490814208984\n",
      "Iteration 61 loss: 2.9070868492126465\n",
      "Iteration 62 loss: 3.727283477783203\n",
      "Iteration 63 loss: 4.64805793762207\n",
      "Iteration 64 loss: 2.7278523445129395\n",
      "Iteration 65 loss: 5.796290397644043\n",
      "Iteration 66 loss: 4.868495941162109\n",
      "Iteration 67 loss: 4.6170878410339355\n",
      "Iteration 68 loss: 2.568856954574585\n",
      "Iteration 69 loss: 4.5789384841918945\n",
      "Iteration 70 loss: 4.507702350616455\n",
      "Iteration 71 loss: 5.954123497009277\n",
      "Iteration 72 loss: 3.872856378555298\n",
      "Iteration 73 loss: 5.501780986785889\n",
      "Iteration 74 loss: 4.177600860595703\n",
      "Iteration 75 loss: 4.332690238952637\n",
      "Iteration 76 loss: 5.645316123962402\n",
      "Iteration 77 loss: 5.8812055587768555\n",
      "Iteration 78 loss: 4.286864280700684\n",
      "Iteration 79 loss: 4.120730876922607\n",
      "Iteration 80 loss: 4.307600021362305\n",
      "Iteration 81 loss: 3.637714385986328\n",
      "Iteration 82 loss: 2.61916446685791\n",
      "Iteration 83 loss: 3.9266133308410645\n",
      "Iteration 84 loss: 4.0866570472717285\n",
      "Iteration 85 loss: 4.730258941650391\n",
      "Iteration 86 loss: 3.644023895263672\n",
      "Iteration 87 loss: 5.430071830749512\n",
      "Iteration 88 loss: 4.61878776550293\n",
      "Iteration 89 loss: 4.571707248687744\n",
      "Iteration 90 loss: 5.238160610198975\n",
      "Iteration 91 loss: 3.635657548904419\n",
      "Iteration 92 loss: 5.809516906738281\n",
      "Iteration 93 loss: 4.320193290710449\n",
      "Iteration 94 loss: 4.681912899017334\n",
      "Iteration 95 loss: 4.480632781982422\n",
      "Iteration 96 loss: 2.316225051879883\n",
      "Iteration 97 loss: 3.2580761909484863\n",
      "Iteration 98 loss: 5.707735061645508\n",
      "Iteration 99 loss: 4.350409030914307\n",
      "Iteration 100 loss: 2.945002317428589\n",
      "Iteration 101 loss: 4.236527442932129\n",
      "Iteration 102 loss: 2.7409470081329346\n",
      "Iteration 103 loss: 3.6240055561065674\n",
      "Iteration 104 loss: 5.188150405883789\n",
      "Iteration 105 loss: 4.1479387283325195\n",
      "Iteration 106 loss: 3.99755859375\n",
      "Iteration 107 loss: 3.426027297973633\n",
      "Iteration 108 loss: 3.4707717895507812\n",
      "Iteration 109 loss: 4.568658828735352\n",
      "Iteration 110 loss: 4.228534698486328\n",
      "Iteration 111 loss: 6.13916015625\n",
      "Iteration 112 loss: 3.8646650314331055\n",
      "Iteration 113 loss: 5.00120210647583\n",
      "Iteration 114 loss: 3.4014861583709717\n",
      "Iteration 115 loss: 3.7405550479888916\n",
      "Iteration 116 loss: 4.360352516174316\n",
      "Iteration 117 loss: 4.151319980621338\n",
      "Iteration 118 loss: 4.0532660484313965\n",
      "Iteration 119 loss: 4.574272632598877\n",
      "Iteration 120 loss: 4.172616958618164\n",
      "Iteration 121 loss: 3.4026641845703125\n",
      "Iteration 122 loss: 4.495787143707275\n",
      "Iteration 123 loss: 3.5749828815460205\n",
      "Iteration 124 loss: 3.2682852745056152\n",
      "Iteration 125 loss: 3.268110513687134\n",
      "Iteration 126 loss: 4.3795294761657715\n",
      "Iteration 127 loss: 3.6514382362365723\n",
      "Iteration 128 loss: 4.436269760131836\n",
      "Iteration 129 loss: 4.512592792510986\n",
      "Iteration 130 loss: 3.460319757461548\n",
      "Iteration 131 loss: 4.140451431274414\n",
      "Iteration 132 loss: 5.764726638793945\n",
      "Iteration 133 loss: 4.187904357910156\n",
      "Iteration 134 loss: 4.256764888763428\n",
      "Iteration 135 loss: 4.2789998054504395\n",
      "Iteration 136 loss: 2.9184153079986572\n",
      "Iteration 137 loss: 3.4854626655578613\n",
      "Iteration 138 loss: 3.438600540161133\n",
      "Iteration 139 loss: 4.66838264465332\n",
      "Iteration 140 loss: 3.776045799255371\n",
      "Iteration 141 loss: 5.051727294921875\n",
      "Iteration 142 loss: 3.7391929626464844\n",
      "Iteration 143 loss: 3.937082052230835\n",
      "Iteration 144 loss: 4.493934631347656\n",
      "Iteration 145 loss: 3.8763906955718994\n",
      "Iteration 146 loss: 3.8073487281799316\n",
      "Iteration 147 loss: 3.025102138519287\n",
      "Iteration 148 loss: 3.636969804763794\n",
      "Iteration 149 loss: 4.247581958770752\n",
      "Iteration 150 loss: 4.893164157867432\n",
      "Iteration 151 loss: 5.7535905838012695\n",
      "Iteration 152 loss: 3.6644444465637207\n",
      "Iteration 153 loss: 3.5792367458343506\n",
      "Iteration 154 loss: 2.731694221496582\n",
      "Iteration 155 loss: 3.6670217514038086\n",
      "Iteration 156 loss: 3.2761833667755127\n",
      "Iteration 157 loss: 3.6646571159362793\n",
      "Iteration 158 loss: 2.8435792922973633\n",
      "Iteration 159 loss: 4.348848342895508\n",
      "Iteration 160 loss: 4.712433815002441\n",
      "Iteration 161 loss: 3.771153450012207\n",
      "Iteration 162 loss: 4.937126159667969\n",
      "Iteration 163 loss: 3.2583072185516357\n",
      "Iteration 164 loss: 4.9765214920043945\n",
      "Iteration 165 loss: 2.6614041328430176\n",
      "Iteration 166 loss: 5.763522624969482\n",
      "Iteration 167 loss: 4.320369243621826\n",
      "Iteration 168 loss: 4.026182174682617\n",
      "Iteration 169 loss: 4.311191082000732\n",
      "Iteration 170 loss: 2.8346381187438965\n",
      "Iteration 171 loss: 3.7704579830169678\n",
      "Iteration 172 loss: 3.7361135482788086\n",
      "Iteration 173 loss: 3.279196262359619\n",
      "Iteration 174 loss: 4.46868371963501\n",
      "Iteration 175 loss: 3.1275362968444824\n",
      "Iteration 176 loss: 4.174958229064941\n",
      "Iteration 177 loss: 4.951388835906982\n",
      "Iteration 178 loss: 3.68886137008667\n",
      "Iteration 179 loss: 3.9783852100372314\n",
      "Iteration 180 loss: 4.694244384765625\n",
      "Iteration 181 loss: 3.190624952316284\n",
      "Iteration 182 loss: 4.086446285247803\n",
      "Iteration 183 loss: 3.832522392272949\n",
      "Iteration 184 loss: 3.736508846282959\n",
      "Iteration 185 loss: 4.034839153289795\n",
      "Iteration 186 loss: 4.3768768310546875\n",
      "Iteration 187 loss: 3.4925055503845215\n",
      "Iteration 188 loss: 2.676095485687256\n",
      "Iteration 189 loss: 4.036802291870117\n",
      "Iteration 190 loss: 2.1481144428253174\n",
      "Iteration 191 loss: 2.8554890155792236\n",
      "Iteration 192 loss: 4.451372146606445\n",
      "Iteration 193 loss: 4.92869234085083\n",
      "Iteration 194 loss: 4.021274089813232\n",
      "Iteration 195 loss: 2.0159718990325928\n",
      "Iteration 196 loss: 4.136082172393799\n",
      "Iteration 197 loss: 3.779616117477417\n",
      "Iteration 198 loss: 4.053213596343994\n",
      "Iteration 199 loss: 3.128042221069336\n"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
