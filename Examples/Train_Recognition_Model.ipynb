{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Imports",
   "id": "7b45a96c1ea6cee2"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import json\n",
    "import os\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import Literal\n",
    "\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from Benchmarks.ModelTester import ModelTester\n",
    "from Datasets.base import DatasetMode\n",
    "from Datasets.batching import BatchManager"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Parameter Setup",
   "id": "594e4890ab4db02f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Choose a trained dataset and model\n",
    "dataset_name: Literal[\"ncaltech\", \"ncars\"] = \"ncars\"\n",
    "model_name: Literal[\"AEGNN\", \"AEGNN-EVGNN\", \"EVGNN\", \"EGSST\"] = \"EVGNN\"\n",
    "\n",
    "# Set corresponding paths\n",
    "dataset_path: Path = Path(r\"D:\\Uniwersytet\\GNNBenchmarking\\Datasets\\NCars\")\n",
    "pretrained_model_path: Path | None = None\n",
    "results_path: Path = Path(r\"../Results\") / f\"Recognition_{model_name}_on_{dataset_name}\"\n",
    "\n",
    "# Training parameters\n",
    "trained_mode: DatasetMode = \"training\"\n",
    "epoch_count: int = 500\n",
    "batch_size: int = 8\n",
    "\n",
    "learning_rate: float = 4e-3\n",
    "scheduler_patience: int = 100\n",
    "scheduler_factor: float = 0.5\n",
    "\n",
    "saving_frequency: int = 100\n",
    "\n",
    "# Graph preprocessing parameters\n",
    "inference_event_count: int = 10000\n",
    "beta: float = 0.0001\n",
    "radius: float = 3.\n",
    "\n",
    "# Performance Analysis parameters\n",
    "sampled_graphs: int = 50\n",
    "batch_sizes: list[int] = [1, 2, 4, 8]\n",
    "test_sizes: list[int] = [100, 100, 100, 100]\n",
    "detail_model_parameters: bool = False\n",
    "\n",
    "# EGSST specific parameters\n",
    "min_nodes_subgraph: int = 1000\n",
    "ecnn_flag: bool = True # Whether enhanced cnn should be used\n",
    "ti_flag: bool = True # Whether TAC augmentation should be used\n",
    "\n",
    "# AEGNN specific parameters\n",
    "kernel_size: int = 8\n",
    "pooling_outputs: int = 128\n",
    "max_neighbors: int = 32\n",
    "sampling: bool = True"
   ],
   "id": "b1168ca1b091c389",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Dataset Initialization",
   "id": "241ff2e339cc99df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if dataset_name == \"ncars\":\n",
    "    from Datasets.ncars import NCars\n",
    "    dataset = NCars(\n",
    "        root = dataset_path\n",
    "    )\n",
    "elif dataset_name == \"ncaltech\":\n",
    "    from Datasets.ncaltech101 import NCaltech\n",
    "    dataset = NCaltech(\n",
    "        root = dataset_path\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(f\"Dataset {dataset_name} not implemented.\")\n",
    "\n",
    "dataset.process()\n",
    "print(f\"Dataset Initialized.\")\n",
    "\n",
    "batch_manager = BatchManager(\n",
    "    dataset = dataset,\n",
    "    batch_size = batch_size,\n",
    "    mode = trained_mode\n",
    ")"
   ],
   "id": "ce08b96edbce8c24",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model Initialized",
   "id": "cb844292a0668c03"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if model_name == \"AEGNN\":\n",
    "    from src.Models.CleanAEGNN.GraphRes import GraphRes as AEGNN\n",
    "    model: AEGNN = AEGNN(\n",
    "        input_shape = (*dataset.get_info().image_size, 3),\n",
    "        kernel_size = kernel_size,\n",
    "        n = [1, 16, 32, 32, 32, 128, 128, 128],\n",
    "        pooling_outputs = pooling_outputs,\n",
    "        num_outputs = len(dataset.get_info().classes),\n",
    "    )\n",
    "\n",
    "    graph_transform = partial(\n",
    "        model.data_transform,\n",
    "        n_samples = inference_event_count,\n",
    "        sampling = sampling,\n",
    "        beta =  beta,\n",
    "        radius = radius,\n",
    "        max_neighbors = max_neighbors\n",
    "    )\n",
    "elif model_name == \"EVGNN\":\n",
    "    from src.Models.CleanEvGNN.GraphRes_Base import GraphRes as EvGNN\n",
    "    model: EvGNN = EvGNN(\n",
    "        input_shape = torch.tensor([*dataset.get_info().image_size, 3]),\n",
    "        dataset = dataset_name,\n",
    "        num_outputs = len(dataset.get_info().classes),\n",
    "        conv_type = \"fuse\",\n",
    "        distill = False,  # <– no KD, just normal training\n",
    "    )\n",
    "\n",
    "    graph_transform = partial(\n",
    "        model.data_transform,\n",
    "        n_samples=inference_event_count,\n",
    "        sampling=sampling,\n",
    "        beta=beta,\n",
    "        radius=radius,\n",
    "        max_neighbors=max_neighbors\n",
    "    )\n",
    "elif model_name == \"AEGNN-EVGNN\":\n",
    "    from src.Models.CleanEvGNN.GraphRes_Base import GraphRes as EvGNN\n",
    "    model: EvGNN = EvGNN(\n",
    "        input_shape = torch.tensor([*dataset.get_info().image_size, 3]),\n",
    "        dataset = dataset_name,\n",
    "        num_outputs = len(dataset.get_info().classes),\n",
    "        conv_type = 'ori_aegnn',\n",
    "        distill = False,  # <– no KD, just normal training\n",
    "    )\n",
    "\n",
    "    graph_transform = partial(\n",
    "        model.data_transform,\n",
    "        n_samples=inference_event_count,\n",
    "        sampling=sampling,\n",
    "        beta=beta,\n",
    "        radius=radius,\n",
    "        max_neighbors=max_neighbors\n",
    "    )\n",
    "elif model_name == \"EGSST\":\n",
    "    from Models.EGSST.EGSST import EGSST\n",
    "\n",
    "    model: EGSST = EGSST(\n",
    "        dataset_information = dataset.get_info(),\n",
    "        detection_head_config = \"\",\n",
    "        task = \"cls\"\n",
    "    )\n",
    "\n",
    "    def transform_graph(graph):\n",
    "        graph.x = graph.x[:inference_event_count, :]\n",
    "        graph.pos = graph.pos[:inference_event_count, :]\n",
    "        graph = model.data_transform(graph, beta = beta, radius = radius, min_nodes_subgraph = min_nodes_subgraph)\n",
    "\n",
    "        if graph is None or graph.pos is None:\n",
    "            return None\n",
    "\n",
    "        return graph\n",
    "\n",
    "    graph_transform = transform_graph\n",
    "else:\n",
    "    raise ValueError(f\"Model {model_name} not implemented.\")\n",
    "\n",
    "if pretrained_model_path is not None:\n",
    "    print(f\"Loading model from: {pretrained_model_path}\")\n",
    "    state_dict = torch.load(pretrained_model_path, weights_only = True)\n",
    "    model.load_state_dict(state_dict)\n",
    "    print(\"Model Loaded.\")\n",
    "\n",
    "dataset.transform = graph_transform\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "d55ecc145a55a0d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Performance Check",
   "id": "a2d0c9d085740b8b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_tester = ModelTester(\n",
    "    results_path = results_path,\n",
    "    model = model\n",
    ")\n",
    "\n",
    "model_hyperparameters = {\n",
    "    \"Data Preprocessing\" : {\n",
    "        \"inference_event_count\": inference_event_count,\n",
    "        \"beta\": beta,\n",
    "        \"radius\": radius\n",
    "    }\n",
    "}\n",
    "\n",
    "if model_name == \"EGSST\":\n",
    "    model_hyperparameters[\"Data Preprocessing\"][\"min_nodes_subgraph\"] = min_nodes_subgraph\n",
    "    model_hyperparameters.update({\n",
    "        \"EGSST Internal\": {\n",
    "            \"ecnn_flag\": ecnn_flag,\n",
    "            \"ti_flag\": ti_flag,\n",
    "        }\n",
    "    })\n",
    "\n",
    "if model_name == \"AEGNN\" or model_name == \"AEGNN-EVGNN\":\n",
    "    model_hyperparameters[\"Data Preprocessing\"][\"sampling\"] = sampling\n",
    "    model_hyperparameters[\"Data Preprocessing\"][\"max_neighbors\"] = max_neighbors\n",
    "\n",
    "if model_name == \"AEGNN\":\n",
    "    model_hyperparameters.update({\n",
    "        \"AEGNN Internal\": {\n",
    "            \"kernel_size\": kernel_size,\n",
    "            \"pooling_outputs\": pooling_outputs,\n",
    "        }\n",
    "    })\n",
    "\n",
    "print(\"Recording Model's Hyperparameters...\")\n",
    "model_tester.record_model_hyperparameters(model_hyperparameters)\n",
    "\n",
    "if model_name != \"AEGNN\":\n",
    "    print(\"Assessing Model's performance Metrics...\")\n",
    "    model_tester.test_model_performance(\n",
    "        dataset = dataset,\n",
    "        mode = trained_mode,\n",
    "        sampled_count = sampled_graphs,\n",
    "        batch_sizes = batch_sizes,\n",
    "        test_sizes = test_sizes,\n",
    "        device = device\n",
    "    )\n",
    "\n",
    "if detail_model_parameters:\n",
    "    print(\"Detailing Model's Parameters...\")\n",
    "    model_tester.detail_model_parameters()"
   ],
   "id": "9be68f8e00800b4f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training Setup",
   "id": "9fd2fbe9cadbc944"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "optimizer = AdamW(model.parameters(), lr = learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=scheduler_factor, patience=scheduler_patience\n",
    ")\n",
    "loss_fn = CrossEntropyLoss()\n",
    "\n",
    "os.makedirs(results_path / \"TrainedModels\", exist_ok = True)"
   ],
   "id": "26c596a5c52a690c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training",
   "id": "1ce42d2064240a25"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model.train()\n",
    "\n",
    "losses = []\n",
    "best_loss = float(\"inf\")\n",
    "epoch_of_best_loss = 0\n",
    "with model_tester:\n",
    "    for i in range(1, epoch_count + 1):\n",
    "        batch = next(batch_manager)\n",
    "        batch = batch.to(device)\n",
    "        reference = batch.y.long()\n",
    "\n",
    "        out = model(batch)\n",
    "        loss = loss_fn(out, reference)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step(loss.item())\n",
    "\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            epoch_of_best_loss = i\n",
    "\n",
    "        print(f\"Epoch {i} | Learning Rate: {optimizer.param_groups[0]['lr']:.2e} | Epoch of Best Loss: {epoch_of_best_loss} | Loss: {loss.item():.2e}\")\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if i % saving_frequency == 0 or i == epoch_of_best_loss or i == epoch_count:\n",
    "            torch.save(model.state_dict(), results_path / f\"TrainedModels\" / f\"{i}.pth\")\n",
    "\n",
    "with open(results_path / \"training_loss_log.json\", 'w') as f:\n",
    "    json.dump(losses, f, indent=4)"
   ],
   "id": "bdedcdc0bf2be4eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Power Consumption",
   "id": "abeb3ca0997bb384"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model_tester.print_power_consumption()",
   "id": "fd046848166c1936",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
