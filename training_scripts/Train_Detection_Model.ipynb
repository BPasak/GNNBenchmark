{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Imports",
   "id": "ccd9acd2cbd1ac01"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Literal\n",
    "\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from Benchmarks.ModelTester import ModelTester\n",
    "from Datasets.base import DatasetMode\n",
    "from Datasets.batching import BatchManager"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Parameter Setup",
   "id": "94956cbe1ef35103"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Choose a trained dataset and model\n",
    "dataset_name: Literal[\"NCaltech101\", \"Gen1\"] = \"Gen1\"\n",
    "model_name: Literal[\"EGSST\", \"AEGNN\"] = \"EGSST\"\n",
    "\n",
    "# Set corresponding paths\n",
    "dataset_path: Path = Path(r\"D:\\Uniwersytet\\GNNBenchmarking\\Datasets\\GEN1-DS\")\n",
    "pretrained_model_path: Path | None = None\n",
    "results_path: Path = Path(r\"../Results\") / f\"Detection_{model_name}_on_{dataset_name}\"\n",
    "\n",
    "# Training parameters\n",
    "trained_mode: DatasetMode = \"validation\"\n",
    "epoch_count: int = 1000\n",
    "batch_size: int = 8\n",
    "\n",
    "learning_rate: float = 4e-4\n",
    "scheduler_patience: int = 100\n",
    "scheduler_factor: float = 0.5\n",
    "\n",
    "saving_frequency: int = 100\n",
    "\n",
    "# Graph preprocessing parameters\n",
    "inference_event_count: int = 10000\n",
    "beta: float = 0.0001\n",
    "radius: float = 5.\n",
    "\n",
    "# Performance Analysis parameters\n",
    "sampled_graphs: int = 50\n",
    "batch_sizes: list[int] = [1, 2, 4, 8]\n",
    "test_sizes: list[int] = [100, 100, 100, 100]\n",
    "detail_model_parameters: bool = False\n",
    "\n",
    "# EGSST specific parameters\n",
    "min_nodes_subgraph: int = 1000\n",
    "detection_head_config: Path = Path(r\"../confs/rtdtr_head_gen1.yml\")\n",
    "ecnn_flag: bool = True # Whether enhanced cnn should be used\n",
    "ti_flag: bool = True # Whether TAC augmentation should be used\n",
    "\n",
    "# AEGNN specific parameters\n",
    "kernel_size: int = 8\n",
    "pooling_outputs: int = 128\n",
    "max_neighbors: int = 32\n",
    "sampling: bool = True"
   ],
   "id": "5baa6e5ca285329b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Loading Dataset",
   "id": "a5de4e471df219db"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f\"Loading {dataset_name} dataset...\")\n",
    "if dataset_name == \"Gen1\":\n",
    "    from Datasets.gen1 import Gen1\n",
    "    dataset = Gen1(\n",
    "        root=dataset_path,\n",
    "    )\n",
    "elif dataset_name == \"NCaltech101\":\n",
    "    from Datasets.ncaltech101 import NCaltech\n",
    "    dataset = NCaltech(\n",
    "        root=dataset_path,\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(f\"Dataset {dataset_name} not implemented.\")\n",
    "\n",
    "dataset.process()\n",
    "print(f\"Dataset Initialized.\")\n",
    "\n",
    "batch_manager = BatchManager(\n",
    "    dataset = dataset,\n",
    "    batch_size = batch_size,\n",
    "    mode = trained_mode\n",
    ")"
   ],
   "id": "32e6900b2a9f1f94",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Initializing Model",
   "id": "693e25c9eb025e67"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f\"Initializing {model_name} model...\")\n",
    "if model_name == \"EGSST\":\n",
    "    from Models.EGSST.EGSST import EGSST\n",
    "    model = EGSST(\n",
    "        dataset_information = dataset.get_info(),\n",
    "        detection_head_config = str(detection_head_config)\n",
    "    )\n",
    "\n",
    "    def transform_graph(graph):\n",
    "        graph.x = graph.x[:inference_event_count, :]\n",
    "        graph.pos = graph.pos[:inference_event_count, :]\n",
    "        graph = model.data_transform(graph, beta = beta, radius = radius, min_nodes_subgraph = min_nodes_subgraph)\n",
    "\n",
    "        if graph is None or graph.pos is None:\n",
    "            return None\n",
    "\n",
    "        if isinstance(graph.bbox, dict):\n",
    "            maximum_time = graph.pos[:, 2].max()\n",
    "            times_of_boxes = torch.tensor(list(graph.bbox.keys()))\n",
    "            time_diff = (times_of_boxes - maximum_time) ** 2\n",
    "            graph.bbox = graph.bbox[list(graph.bbox.keys())[time_diff.argmin()]]\n",
    "\n",
    "        return graph\n",
    "\n",
    "    graph_transform = transform_graph\n",
    "elif model_name == \"AEGNN\":\n",
    "    from Models.CleanAEGNN.AEGNN_Detection import AEGNN_Detection\n",
    "    model = AEGNN_Detection(\n",
    "        input_shape = (*dataset.get_info().image_size, 3),\n",
    "        kernel_size = kernel_size,\n",
    "        n = [1, 16, 32, 32, 32, 128, 128, 128],\n",
    "        pooling_outputs = pooling_outputs,\n",
    "        num_classes = len(dataset.get_info().classes),\n",
    "    )\n",
    "\n",
    "    def transform_graph(graph):\n",
    "        graph = model.data_transform(\n",
    "            graph, n_samples = inference_event_count, sampling = sampling,\n",
    "            beta =  beta, radius = radius,\n",
    "            max_neighbors = max_neighbors\n",
    "        )\n",
    "\n",
    "        if isinstance(graph.bbox, dict):\n",
    "            maximum_time = graph.pos[:, 2].max()\n",
    "            times_of_boxes = torch.tensor(list(graph.bbox.keys()))\n",
    "            time_diff = (times_of_boxes - maximum_time) ** 2\n",
    "            graph.bbox = graph.bbox[list(graph.bbox.keys())[time_diff.argmin()]]\n",
    "\n",
    "        return graph\n",
    "\n",
    "    graph_transform = transform_graph\n",
    "else:\n",
    "    raise ValueError(f\"Model {model_name} not implemented.\")\n",
    "print(f\"Model Initialized.\")\n",
    "\n",
    "if pretrained_model_path is not None:\n",
    "    print(f\"Loading model from: {pretrained_model_path}\")\n",
    "    state_dict = torch.load(pretrained_model_path, map_location=torch.device(\"cpu\"))\n",
    "    model = model.load_state_dict(state_dict)\n",
    "    print(\"Model Loaded.\")\n",
    "\n",
    "dataset.transform = graph_transform\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "29b2c587960edcb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Performance Check",
   "id": "a641fbf84936ce13"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_tester = ModelTester(\n",
    "    results_path = results_path,\n",
    "    model = model\n",
    ")\n",
    "\n",
    "model_hyperparameters = {\n",
    "    \"Data Preprocessing\" : {\n",
    "        \"inference_event_count\": inference_event_count,\n",
    "        \"beta\": beta,\n",
    "        \"radius\": radius\n",
    "    }\n",
    "}\n",
    "\n",
    "if model_name == \"EGSST\":\n",
    "    model_hyperparameters[\"Data Preprocessing\"][\"min_nodes_subgraph\"] = min_nodes_subgraph\n",
    "    model_hyperparameters.update({\n",
    "        \"EGSST Internal\": {\n",
    "            \"detection_head_config\": detection_head_config,\n",
    "            \"ecnn_flag\": ecnn_flag,\n",
    "            \"ti_flag\": ti_flag,\n",
    "        }\n",
    "    })\n",
    "elif model_name == \"AEGNN\":\n",
    "    model_hyperparameters[\"Data Preprocessing\"][\"sampling\"] = sampling\n",
    "    model_hyperparameters[\"Data Preprocessing\"][\"max_neighbors\"] = max_neighbors\n",
    "    model_hyperparameters.update({\n",
    "        \"AEGNN Internal\": {\n",
    "            \"kernel_size\": kernel_size,\n",
    "            \"pooling_outputs\": pooling_outputs,\n",
    "        }\n",
    "    })\n",
    "\n",
    "print(\"Recording Model's Hyperparameters...\")\n",
    "model_tester.record_model_hyperparameters(model_hyperparameters)\n",
    "\n",
    "if model_name != \"AEGNN\":\n",
    "    print(\"Assessing Model's performance Metrics...\")\n",
    "    model_tester.test_model_performance(\n",
    "        dataset = dataset,\n",
    "        mode = trained_mode,\n",
    "        sampled_count = sampled_graphs,\n",
    "        batch_sizes = batch_sizes,\n",
    "        test_sizes = test_sizes,\n",
    "        device = device\n",
    "    )\n",
    "\n",
    "if detail_model_parameters:\n",
    "    print(\"Detailing Model's Parameters...\")\n",
    "    model_tester.detail_model_parameters()"
   ],
   "id": "d343be508f6e008a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training Setup",
   "id": "9e25e393047dc7c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "optimizer = AdamW(model.parameters(), lr = learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=scheduler_factor, patience=scheduler_patience\n",
    ")\n",
    "\n",
    "os.makedirs(results_path / \"TrainedModels\", exist_ok = True)"
   ],
   "id": "1565d35886e643d6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training",
   "id": "6e7f63229ae86cc9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "loss_lists = {}\n",
    "best_loss = float(\"inf\")\n",
    "epoch_of_best_loss = 0\n",
    "\n",
    "with model_tester:\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(1, epoch_count + 1):\n",
    "        batch = next(batch_manager)\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # ---- forward + backward ----\n",
    "        optimizer.zero_grad()\n",
    "        total_loss, loss_dict = model(batch)\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step(total_loss)\n",
    "\n",
    "        epoch_loss = total_loss.item()\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            epoch_of_best_loss = epoch\n",
    "\n",
    "        print(f\"Epoch: {epoch} | Learning Rate: {optimizer.param_groups[0]['lr']}\")\n",
    "        print(f\"Total loss: {epoch_loss:.4f} | Epoch of best loss: {epoch_of_best_loss}\")\n",
    "        for loss in [\"loss_bbox\", \"loss_giou\", \"loss_ce\"]:\n",
    "            if loss in loss_dict:\n",
    "                print(f\"  - {loss}: {loss_dict[loss].item():.4f}\")\n",
    "\n",
    "            if loss not in loss_lists:\n",
    "                loss_lists[loss] = []\n",
    "\n",
    "            loss_lists[loss].append(loss_dict[loss].item())\n",
    "\n",
    "        if epoch % saving_frequency == 0 or epoch == epoch_of_best_loss or epoch == epoch_count:\n",
    "            torch.save(model.state_dict(), results_path / \"TrainedModels\" / f\"{epoch}.pth\")\n",
    "\n",
    "with open(results_path / \"training_loss_log.json\", 'w') as f:\n",
    "    json.dump(loss_lists, f, indent=4)"
   ],
   "id": "a1c01a8c86dd1ad8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Power Consumption",
   "id": "d099cdd0c0e6758a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model_tester.print_power_consumption()",
   "id": "a4c8d8cc1104c6e0",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
